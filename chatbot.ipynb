{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OCrEcaDiQNCO"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "# !pip install numpy\n",
    "# !pip install tensorflow\n",
    "import os\n",
    "# print(os.getcwd())\n",
    "# print(os.listdir())\n",
    "import csv\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Reading and preprocessing the data\n",
    "\n",
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        tokenized_words = nltk.word_tokenize(pattern)\n",
    "       # print(tokenized_words)\n",
    "        words.extend(tokenized_words)\n",
    "        docs_x.append(tokenized_words)\n",
    "        docs_y.append(intent['tag'])\n",
    "\n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w != '?']\n",
    "words = sorted(list(set(words)))\n",
    "labels = sorted(labels)\n",
    "\n",
    "# Step 2: Creating training and testing data\n",
    "training_data = []\n",
    "output_data = []\n",
    "out_empty = [0] * len(labels)\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    for w in words:\n",
    "        if w in doc:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training_data.append(bag)\n",
    "    output_data.append(output_row)\n",
    "\n",
    "training_data = np.array(training_data)\n",
    "output_data = np.array(output_data)\n",
    "\n",
    "# Step 3: Creating the model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(training_data[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(output_data[0]), activation='softmax'))\n",
    "\n",
    "# Step 4: Compiling the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Training the model\n",
    "model.fit(training_data, output_data, epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "# Step 6: Saving the model\n",
    "model.save('chatbot_model.h5')\n",
    "\n",
    "# Step 7: Loading the model\n",
    "model = tf.keras.models.load_model('chatbot_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "def calculate_attendance_percentage():\n",
    "    file_path = 'attendance.csv'\n",
    "    student_id = input('Enter the rollno :-')\n",
    "    total_classes = 0\n",
    "    attended_classes = 0\n",
    "\n",
    "    with open(file_path, 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file)\n",
    "        header = next(csv_reader)  # Skip the header row\n",
    "\n",
    "        # Assuming the CSV file has columns: StudentID, Date, Attendance\n",
    "        for row in csv_reader:\n",
    "            if row[0] == student_id:\n",
    "                total_classes += 1\n",
    "                if row[2].lower() == 'present':\n",
    "                    attended_classes += 1\n",
    "\n",
    "    if total_classes == 0:\n",
    "        return 0.0\n",
    "\n",
    "    attendance_percentage = (attended_classes / total_classes) * 100\n",
    "    \n",
    "    print(attendance_percentage)\n",
    "\n",
    "\n",
    "def print_timetable():\n",
    "    \n",
    "    day = input(\"Enter the day: \")\n",
    "    department = input(\"Enter the department: \")\n",
    "    with open('timetable.csv', 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row[0].lower() == day.lower() and row[3].lower() == department.lower():\n",
    "                print(f\"Day: {row[0]}\\nTime Slot: {row[1]}\\nSubject: {row[2]}\\nDepartment: {row[3]}\\n\")\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 8: Defining the chatbot function\n",
    "def chatbot():\n",
    "    print(\"Welcome to the chatbot! Start talking with the bot (type 'quit' to stop):\")\n",
    "    while True:\n",
    "        user_input  = input(\"You :\")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        tokenized_words = nltk.word_tokenize(user_input)\n",
    "        tokenized_words = [lemmatizer.lemmatize(w.lower()) for w in tokenized_words]\n",
    "        \n",
    "        input_data = np.array([0] * len(words))\n",
    "        target_words = ['attendance', 'timetable']\n",
    "        for word in tokenized_words:\n",
    "            if word == target_words[0]:\n",
    "                calculate_attendance_percentage()\n",
    "            elif word == target_words[1]:\n",
    "                print_timetable()\n",
    "            else:\n",
    "                break\n",
    "        for w in tokenized_words:\n",
    "            if w in words:\n",
    "                input_data[words.index(w)] = 1\n",
    "\n",
    "        results = model.predict(np.array([input_data]))\n",
    "        results_index = np.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        for intent in data['intents']:\n",
    "            if intent['tag'] == tag:\n",
    "                responses = intent['responses']\n",
    "\n",
    "        print(random.choice(responses))\n",
    "\n",
    "# Running the chatbot function\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def generate_dummy_timetable():\n",
    "    days_of_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "    time_slots = ['9:00 AM - 10:30 AM', '10:45 AM - 12:15 PM', '1:00 PM - 2:30 PM', '2:45 PM - 4:15 PM', '4:30 PM - 6:00 PM']\n",
    "    subjects = ['Mathematics', 'Physics', 'Chemistry', 'Computer Science', 'English', 'History']\n",
    "    departments = ['Department A', 'Department B', 'Department C', 'Department D']\n",
    "\n",
    "    timetable = []\n",
    "\n",
    "    for day in days_of_week:\n",
    "        for time_slot in time_slots:\n",
    "            subject = random.choice(subjects)\n",
    "            department = random.choice(departments)\n",
    "\n",
    "            timetable.append([day, time_slot, subject, department])\n",
    "\n",
    "    return timetable\n",
    "\n",
    "def save_timetable_to_csv(file_path, timetable):\n",
    "    with open(file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['Day', 'Time Slot', 'Subject', 'Department'])\n",
    "        csv_writer.writerows(timetable)\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'timetable.csv'\n",
    "dummy_timetable = generate_dummy_timetable()\n",
    "save_timetable_to_csv(file_path, dummy_timetable)\n",
    "print(f\"Dummy timetable saved to '{file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "def generate_dummy_dataset(num_students, num_records):\n",
    "    attendance_records = []\n",
    "\n",
    "    for _ in range(num_records):\n",
    "        student_id = f'21bce{random.randint(1, num_students)}'\n",
    "        date = f'{random.randint(1, 12)}/{random.randint(1, 28)}/2023'\n",
    "        attendance_status = random.choice(['Present', 'Absent'])\n",
    "        attendance_records.append([student_id, date, attendance_status])\n",
    "\n",
    "    return attendance_records\n",
    "\n",
    "def save_dataset_to_csv(file_path, dataset):\n",
    "    with open(file_path, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(['StudentID', 'Date', 'Attendance'])\n",
    "        csv_writer.writerows(dataset)\n",
    "\n",
    "# Example usage:\n",
    "num_students = 50\n",
    "num_records = 100\n",
    "file_path = 'attendance.csv'\n",
    "\n",
    "attendance_dataset = generate_dummy_dataset(num_students, num_records)\n",
    "save_dataset_to_csv(file_path, attendance_dataset)\n",
    "print(f\"Dummy dataset with {num_records} attendance records saved to '{file_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def print_timetable(day, department):\n",
    "    user_day = input(\"Enter the day: \")\n",
    "    user_department = input(\"Enter the department: \")\n",
    "    with open('timetable.csv', 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row[0].lower() == day.lower() and row[3].lower() == department.lower():\n",
    "                print(f\"Day: {row[0]}\\nTime Slot: {row[1]}\\nSubject: {row[2]}\\nDepartment: {row[3]}\\n\")\n",
    "\n",
    "# Ask for user input\n",
    "\n",
    "\n",
    "# Print the timetable\n",
    "print_timetable(user_day, user_department)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_unimportant_words(sentence):\n",
    "    # nltk.download('stopwords')  # Download stopwords dataset if not already downloaded\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    tokenized_words = nltk.word_tokenize(sentence)\n",
    "    important_words = []\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        if word.lower() not in stop_words:\n",
    "            important_words.append(word)\n",
    "\n",
    "    cleaned_sentence = ' '.join(important_words)\n",
    "\n",
    "    if 'attendance' in important_words:\n",
    "        print(\"The word 'attendance' is present in the important words.\")\n",
    "\n",
    "    return cleaned_sentence\n",
    "sentence=input()\n",
    "remove_unimportant_words(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_blog_from_website(query):\n",
    "    search_results = search(query, num_results=5)  # Perform a Google search and get the top 5 results\n",
    "\n",
    "    blog_content = []  # List to store the extracted blog content\n",
    "\n",
    "    for result in search_results:\n",
    "        try:\n",
    "            response = requests.get(result)  # Fetch the webpage\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')  # Create a BeautifulSoup object to parse the HTML\n",
    "            # Extract the blog content based on the specific HTML structure of the website\n",
    "            # Adjust the CSS selectors according to the structure of the target website\n",
    "            content = soup.select('div.blog-content p')\n",
    "            if content:\n",
    "                blog_content.append(' '.join([p.get_text() for p in content]))  # Append the extracted content to the list\n",
    "        except requests.exceptions.RequestException:\n",
    "            continue\n",
    "\n",
    "    return blog_content\n",
    "\n",
    "# Example usage\n",
    "query = input(\"Enter a query: \")\n",
    "blog_content = read_blog_from_website(query)\n",
    "print(blog_content)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "18f2f34207b95f455b805bda8f36ba5c3096199f4ec20ce124a4e8043d265747"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
